[linear]
description = """
**Линейная регрессия** — это математическая модель, которая описывает связь нескольких переменных. 

**Цель линейной регрессии** - 
- Найти коэффициенты уравнения (гиперплоскости) ${y = b_0 + b_1x_1+b_2x_2 ... b_nx_n}$,  которые наилучшим образом описывают данные.
    - ${y}$ - зависимая переменная (в нашем случае ```price```)
    - ${x_1, x_2, ... x_n}$ - независимые переменные (остальные признаки)
"""

learn_code = """
#Побор гиперпараметров с помощью optuna
def optuna_params(objective):
    study = optuna.create_study(direction='minimize')
    study.optimize(objective, n_trials=100)
    best_params = study.best_params
    return best_params

def objective(trial):
    alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)
    model = Lasso(alpha=alpha)
    model.fit(X_train, Y_train)
    y_pred = model.predict(X_test)
    mse = mean_squared_error(Y_test, y_pred)
    return mse 

#Обучение лучшей модели
best_params = optuna_params(objective)
best_model = Lasso(alpha=best_params['alpha'])
best_model.fit(X_train,Y_train)

"""

[br]
description = """
**BaggingRegression** - метод, при котором для регрессии усредняются результаты нескольких моделей.

**Суть:** 
- Базовые модели обучаются на разных подмножествах данных 
- Прогнозы объединяются, что улучшает способность к обобщению
- Снижает риск переобучения
"""

learn_code = """
#Подбор гиперпараметров с помощью RandomizedSearchSV
def randomizedSearchCV_params(model, parameters, X_train, Y_train):
    optimal = RandomizedSearchCV(model, parameters).fit(X_train, Y_train)
    output = formated_params(optimal.best_params_)
    return optimal.best_estimator_

#Определение гиперпараметров
br_params= {
    'n_estimators':[10, 50, 100, 200],
    'max_samples': [0.5, 0.7, 1.0],
    'max_features': [0.5, 0.7, 1.0],
    'bootstrap': [True, False],
    'estimator': [dTR, None]
}

#Лучшая модель
BR = randomizedSearchCV_params(BaggingRegressor(), br_params, X_train, y_train)
"""

[sr]
description = """
**StackingRegression** - метод, который комбинирует предсказания нескольких базовых моделей с помощью **мета-модели** (финального регрессора). 

**Суть метода:** 
- Выбирается набор разных регрессеров, каждый из которых обучается на выборке
- Предсказания всех моделей собираются в новый набор
- Мета модель обучается на этом наборе, чтобы комбинировать предсказания и выдавать финальный результат
"""


learn_code = """
#Подбор гиперпараметров с помощью GridSearchCV
def gridSearchCV_params(model, parameters,X_train, Y_train):
    optimal = GridSearchCV(model, parameters).fit(X_train, Y_train)
    output = formated_params(optimal.best_params_)
    return optimal.best_estimator_

#Определение гиперпараметров
estimators_SR = [
    ('gbr', GradientBoostingRegressor(random_state=42)),
    ('rf', RandomForestRegressor(random_state=42)),
    ('svr', SVR())
]
SR_params = {
    'cv': [3,5],
    'passthrough': [True, False]
}

#Лучшая модель
SR = gridSearchCV_params(StackingRegressor(estimators=estimators_SR, final_estimator=Ridge()), SR_params, X_train, y_train)
"""
[gbr]
description = """
**GradientBoostingRegression** - метод, основанный на градиентном бустинге, который **последовательно** строит слабые модели (деревья решений) для минимизации ошибки.

**Суть метода:**
- Модель начинается с простого предсказания
- На каждой итерации строится новое дерево решений, которое корректирует ошибки предыдующих деревьев (учится на ошибках)
- Ошибки вычисляются как градиент функции потерь
- **Финальное предсказание** - это взвешенная сумма предсказаний всех деревьев

"""
learn_code = """

#Определение гиперпараметров
GBR_params = {
    'n_estimators': [10, 50, 100],
    'learning_rate': [0.01,0.05, 0.1, 0.2],
    'max_depth': [3, 5 ,7],
    'max_features': ['sqrt', 'log2', None]
}

#Лучшая модель, полученная на основе GridSearchSV
GBR = gridSearchCV_params(GradientBoostingRegressor(), GBR_params, X_train, y_train)

"""


[catboost]
description = """
**CatBoostRegressor** - это реализация градиентного бустинга от компании [Яндекс](https://catboost.ai/?ysclid=maflbfun1g968169241), оптимизированная для работы с категориальными признаками.

**Характеристики метода**
- Как в ```GradientBoosting``` **CatBoost** строит последовательность деревьер, минимизируя функцию потерь с помощью градиентного спуска
- Встроенные механизмы борьбы с переобучением
"""

learn_code = """
#Определение гиперпараметров
CBR_params = {
    'iterations': [200, 500,],
    'depth': [3,5,7],
    'learning_rate': [0.01,0,1],
    'l2_leaf_reg': [1, 3],
}

#Лучшая модель, полученная на основе GridSearchSV
CB_Regressor = gridSearchCV_params(CatBoostRegressor(), CBR_params, X_train_mumb, y_train_mumb)
"""

[fcnn]
description = """
**FCNN** - _Fully Connected Neural Network_ полносвязная нейронная сеть (MLP) из библиотеки Scikit-learn.

**Принцип работы**
- Принимает на входном слое данные
- Пропускает их через несколько "скрытых" слоёв нейронов, каждый из которых выполняет простую мат.операцию с весами и передаёт результат дальше
- Выдаёт результат на выходном слое

_Полносвязная_ значит, что нет пропусков в соединениях - все нейроны каждого слоя связаны со всеми нейронами следующего
"""

learn_code = """

# Параметры
params_neural ={
    "hidden_layer_sizes": [20,40, 60],
    "solver": ['lbfgs', 'sgd', 'adam'],
    'activation': ['relu', 'identity', 'logistic', 'tanh'],
}
"""


[metrics]
help = """
**MAE** (Mean Absolute Error) - измеряет _среднюю абсолютную ошибку_ между предсказанными и реальными значениями. Показывает насколько в среднем предсказания модели отклюняются от истинных значений.

**MSE** (Mean Squared Error) - измеряет _среднеквадратичную ошибку_. Более чувствительна к большим ошибкам, чем **MAE**. Полезна там, где нужно минимизировать ошибки.

**RMSE** (Root Mean Squared Error) - квадратный корень из **MSE**. Учитывает как средние, так и большие ошибки.

**MAPE** (Mean Absolute Percentage error) -  измеряет отклонение прогнозов от фактических значений **в процентах**. Является хорошим выбором, когда нужно легко интерпретируемое показание ошибки в процентном отношении.

**${R^2}$**  - измеряет долю дисперсии. Значение ближе к 1 указывает на хорошую модель. Метрика показывает, насколько хорошо модель объясняет вариативность данных. 

"""
